{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf85c851-412d-4698-a9f1-fa71cbca35de",
   "metadata": {},
   "source": [
    "Notebook demonstrating document querying and answer generation using Llamaindex, Pinecone and openai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2967d1-dc91-4895-824a-34e65961c17f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0146234-7c65-4f05-9320-c2e506b4ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce205da-9031-4620-9b29-60c36fdfa299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set api keys\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "PINECONE_KEY = os.environ[\"PINECONE_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10d53b-6fb1-41a1-9223-02a0924fb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c9d4f-700d-4847-8c6b-4419e47519df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_client = Pinecone(api_key=PINECONE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26b6be-b0e5-4907-b2a2-586e3c035778",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"rag-demo\"\n",
    "\n",
    "pinecone_index = pinecone_client.Index(index_name)\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=OpenAIEmbedding())\n",
    "retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a440f817-df63-4459-ba86-0fbe47e2a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"List down some text-to-image models\"\n",
    "candidates = retriever.retrieve(query) # retrieve the close candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c8729-e773-4d07-9520-6feb999ec238",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\\n\".join([x.metadata[\"window\"] for x in candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27166cf4-edf0-4fed-803c-b5e797438802",
   "metadata": {},
   "outputs": [],
   "source": [
    "for candidate in candidates:\n",
    "    print(f\"{candidate.score:.2f}\\t{candidate.text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5170e69-f5ca-41fa-90fc-7b08683516f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Answer the question: {question}. \\n\\n Use only the given context to answer the question. You can format the answer in bullet points. The context is: \\n {context}\"\n",
    "prompt = prompt.format(question=query, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c97c1e-fabf-42f9-b52e-121efa938ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt) # print the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad4dcd-92b4-498f-a108-16f13f614919",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-3.5-turbo\", temperature=0.3, messages=[{\"role\": \"user\", \"content\": prompt}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c29af-a4b3-4ff3-8c24-6fcfb2449956",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af37e0-ad34-4487-b2d0-e64c27c6e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1a5830-11ed-49e0-aff4-d1cfa459440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine retrival and answer generation in a single function\n",
    "def get_answer(query):\n",
    "    candidates = retriever.retrieve(query)\n",
    "    context = \"\\n\\n\\n\".join([x.metadata[\"window\"] for x in candidates]) # using window for context\n",
    "    prompt = \"Answer the question: {question}. \\n\\n Use only the given context to answer the question. You can format the answer in bullet points. The context is: \\n {context}\"\n",
    "    prompt = prompt.format(question=query, context=context)\n",
    "    \n",
    "    response = openai.chat.completions.create(model=\"gpt-3.5-turbo\", temperature=0.3, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e3425-8b64-4cdf-b2b5-63192aa9174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_answer(\"Who are the authors of the report?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d70188-f323-43b8-a17e-ee0b85d31517",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_answer(\"Which is the most in-demand field of AI?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4dc8e3-14af-4c2a-8509-f5a7e2e41b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_answer(\"What is the global investment made in AI In 2022?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196fcb21-9c98-4d25-bf65-4bce0334ba19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
